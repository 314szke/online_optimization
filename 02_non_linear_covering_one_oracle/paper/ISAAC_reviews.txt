----------------------- REVIEW 1 ---------------------
SUBMISSION: 167
TITLE: Online Primal-Dual Algorithm with Predictions for Non-Linear Covering Problems
AUTHORS: Kim Thang Nguyen and Eniko Kevi

----------- Overall evaluation -----------
The paper continues a line of work of combining a worst case (primal-dual) algorithm and predictions.
The most relevant works are [8] and [17]. In these works there is a covering problem in which covering constraints arrive online and should be satisfied upon arrival.
We are also given predictions for the variables that may be non-accurate.
These papers show how to combine the predictions with the well known primal-dual algorithm that has a worst case guarantee using a "trust parameter". The resulting algorithm is competitive with respect to both the worst case and the quality of the solution defined by the predictions.

The current work extends these ideas further to non-linear objective functions, and the final ratios depend on previously defined/used notion of smoothness of the objective function.

The results and the algorithms seem to be straightforward extensions of previous ideas and are not surprising. The setting is somewhat interesting as this problem captures several interesting questions.

The main problem is that in [17] (see page 9 and Appendix B in the arxiv version) there is a simple solution for combining the predictions with any online algorithm. Basically we can just see which algorithm is better and make transition between the solutions carefully. I don't see a reason that this idea works here as well. This makes the machinery in this paper much less interesting. The authors should check first if this is indeed the case.



----------------------- REVIEW 2 ---------------------
SUBMISSION: 167
TITLE: Online Primal-Dual Algorithm with Predictions for Non-Linear Covering Problems
AUTHORS: Kim Thang Nguyen and Eniko Kevi

----------- Overall evaluation -----------
[Results]

This paper studies online non-linear covering problems with predictions. In the online problem, each covering constraint and prediction arrive online at each time stamp, the goal is to update the variables in a monotone manner via incorporating the prediction, such that 1) if the prediction is accurate and feasible, then the objective value of the online solution is close to the prediction objective (consistency), and 2) if the prediction is inaccurate, then the objective value of the online solution is close to the objective obtained by the best pure online algorithm (robustness).

This paper uses the multilinear extension formulation to capture a general class of objective functions, including convex and linear. Under a smoothness assumption for the objective function, the authors present a consistent and robust online algorithm, where the performance is evaluated by the smoothness parameters.

In my opinion, although this result is interesting, it does not quite meet the bar for strong acceptance.

[Techniques]

The approach combines the online primal-dual framework for the covering problem [34] and the primal variable updating rule for the problem with predictions [8].

[Evaluation]

Pro:

- The result applies to some non-trivial objective functions beyond convexity.

- The paper is well-written.

Con:

- The technique heavily relies on previous work [8] and [34].

- The application for online submodular minimization might not be interesting because of the strong submodularity structure. One can use a simple algorithm that considers the prediction and the best online algorithm in parallel. In each time stamp, we pick the solution with the smaller objective value between the prediction and the pure online algorithm. We update our online solution by taking the coordinate-wise maximum between the online solution in the previous time stamp and the solution with the smaller objective value obtained in this time stamp. Since submodular functions are subadditive (or concave), this would imply that we can get an online solution within a constant factor of the prediction and the best online algorithm, which outperforms any choice of our confidence parameter $\ita$. A more detailed description of this simple algorithm is in [17] Appendix B. However, the general framework of this paper is not qualitatively subsumed by this simple algorithm. For th
e problem proposed in this work, the simple algorithm pays a $\lambda$ (smoothness) factor which might not be a constant.

[Other Comments]

- I wonder if this framework can be extended to the setting with “fractional” predictions as in the work [17].

[8] Etienne Bamas, Andreas Maggiori, and Ola Svensson. The primal-dual method for learning augmented algorithms.
[17] Elena Grigorescu, Young-San Lin, Sandeep Silwal, Maoyuan Song, and Samson Zhou. Learning-augmented algorithms for online linear and semidefinite programming.
[34] Nguyen Kim Thang. Online primal-dual algorithms with configuration linear programs.



----------------------- REVIEW 3 ---------------------
SUBMISSION: 167
TITLE: Online Primal-Dual Algorithm with Predictions for Non-Linear Covering Problems
AUTHORS: Kim Thang Nguyen and Eniko Kevi

----------- Overall evaluation -----------
This paper considers the learning-augmented online covering problem. In this problem, a set of n resources is given at the beginning, and covering constraints on these n resources arrive one-by-one in an online fashion. Along with each covering constraint, a predicted solution is additionally given. The objective is to find an n-dimensional (zero-one) vector x that minimizes f(x) subject to the covering constraints, where f is a nonlinear function. The problem incorporates load balancing, energy minimization problems in scheduling, and submodular minimization with covering constraints.
The paper presents a primal-dual algorithm for a fractional version of this problem, and also provide a computational experiment with a routing problem. The algorithm and its analysis are based on Thang [34] and Bamas et al. [8]: the present algorithm first considers the multilinear extension F of the original objective function f and construct a strengthened LP and its dual. The given primal-dual algorithm resembles the one in [34], whereas the prediction augmentation part is similar to that of [8].
While the paper studies a natural nonlinear generalization of an interesting problem, it considers a fractional version of that problem without, for example, giving a rounding procedure. A fractional solution is the final output of the algorithm, and therefore we are faced with the question: what does a "fractional" solution exactly mean and why is that the "right" definition of a fractional version? I believe the following must be addressed prior to the publication of this paper:
1. The first question is, when we say a fractional "solution", what are the feasibility constraints? It seems the LP on page 6 is not the answer, because the given algorithm may not output a fractional solution feasible to that LP. Note that the algorithm only considers the constraints defined by A* (step 6 of Algorithm 1), so it may not satisfy the primal constraints for some A's. Consider for example when a^1_1 = a^1_2 = 0.5+\epsilon, A*=\emptyset, and x_1 = x_2 = 1/(1+2\epsilon). On the other hand, the set of input covering constraints only (without their strengthening) also doesn't seem to be the answer, because the given algorithm competes against the fractional optimum of (or the algorithm's performance is bounded by using the dual of) the LP on page 6. See also line 267.
2. It may be natural to think that, if a fractional solution is meaningful in some problem, the objective f must also be defined over [0,1]^n, rather than {0,1}^n. That said, what is the (primal) objective function under which the claimed performance guarantee is given? Is this f, or the multilinear extension F? Line 304 seems to suggest that it should be F. But that means we are measuring the algorithm's performance with an "artificial" objective that is just the multilinear extension of the grid point values of the original objective.
3. In line 111, the theorem assumes (\lambda, \mu)-local-smoothness, which is different from the statement of the same theorem in line 293. We could of course reparametrize the statement in line 293, but that would result in a different robustness bound.
Comments:
(l27) overestimate?
(l46) I wouldn't say the primal-dual method was "introduced" by Williamson and Shmoys [36] as it has a much longer history.
(l229, dual LP) Why are \beta_e and \gamma sign-constrained?
(l232) "constraint": a typo.
(Algorithm 1) Do Lines 7-20 execute simultaneously? The current description is rather ambiguous.
(l262) if -> of



----------------------- REVIEW 4 ---------------------
SUBMISSION: 167
TITLE: Online Primal-Dual Algorithm with Predictions for Non-Linear Covering Problems
AUTHORS: Kim Thang Nguyen and Eniko Kevi

----------- Overall evaluation -----------
A trendy research topic in the last recent years is the design of online algorithms with predictions. In this context, Bamas et al. '20 developed a primal-dual approach for several problems with covering constraints and linear objectives, with a performance guarantee that improves whenever the prediction is more accurate.

The present paper extends this approach for non-linear covering problems (i.e., problems with covering constraints but non-linear objective function).

The authors give an abstract framework, and show its implications on examples ranging from load balancing, to energy minimization in scheduling, and to submodular minimization. Finally, they also evaluate empirically the performance of their proposed algorithm, on some routing problem instances.

Online algorithms with predictions are a very hot topic at the moment. I am not familiar enough with the area to evaluate how much novelty the paper introduces from a technical perspective. Nevertheless, I like the extension to non-linear covering problems and I find the set of applications nice. Overall, I think the paper can be a nice contribution to ISAAC.
