%!TEX root = ./main.tex

\subsection{Norm minimization} \label{appix-norm}

As a first application, we consider problems where the objective function is the $\ell_{k}$-norm. Formally,  $f(\vect{x}) = \|C \vect{x}\|_{k}$, where $C \in \mathbb{R}^{m \times n}$ is a matrix. The corresponding optimization problem is: $\min \|C \vect{x}\|_{k}$ subject to $B \vect{x} \geq 0$. To apply our framework, we first determine the $(\lambda, \mu)$-local-smoothness parameters of $\|C \vect{x}\|_{k}$.

\begin{lemma} \label{lem-makespan}
For $C \in \mathbb{R}^{m \times n}$, function $f(\vect{x}) := \|C \vect{x}\|_{k}$ is $(k,0)$-locally-smooth.
\end{lemma}
%
\begin{proof}
To prove the lemma, we show that for every subset $S \subseteq \{1, \ldots, n\}$ and for every $\vect{x} \in [0,1]^{n}$, the following holds.
%
\begin{align}	\label{ineq:lp-smooth}
\sum_{e \in S} \nabla_{e} f(\vect{x}) \leq  k \cdot \|C \cdot \one_{S}\|_{k}
\end{align}
%
The partial derivative is
%
\begin{align*}
\frac{\partial f(\vect{x})}{\partial x_{e}}
= k \cdot \sum_{i = 1}^{m} \frac{c_{i,e} \cdot \bigl( \sum_{e' = 1}^{n} c_{i,e'} x_{i,e'} \bigr)^{k-1} }{ \left[ \sum_{i' = 1}^{m}
		\bigl( \sum_{e'= 1}^{n} c_{i',e'} x_{i',e'} \bigr)^{k} \right]^{(k-1)/k}},
\end{align*}
so when we sum over $S$, we get
\begin{align*}
\sum_{e \in S} \frac{\partial f(\vect{x})}{\partial x_{e}}
= k \cdot \frac{ \sum_{e \in S} \sum_{i = 1}^{m} c_{i,e} \cdot \bigl( \sum_{e' = 1}^{n} c_{i,e'} x_{i,e'} \bigr)^{k-1} }{ \left[ \sum_{i' = 1}^{m} \bigl( \sum_{e'= 1}^{n} c_{i',e'} x_{i',e'} \bigr)^{k} \right]^{(k-1)/k}}.
\end{align*}
%
Therefore, inequality (\ref{ineq:lp-smooth}) is equivalent to
\begin{align*}
\frac{ \sum_{e \in S} \sum_{i = 1}^{m} c_{i,e} \cdot \bigl( \sum_{e' = 1}^{n} c_{i,e'} x_{i,e'} \bigr)^{k-1} }{ \left[ \sum_{i' = 1}^{m} \bigl( \sum_{e'= 1}^{n} c_{i',e'} x_{i',e'} \bigr)^{k} \right]^{(k-1)/k}}
	\leq   \biggl \| \biggl( \sum_{e: e \in S} c_{1,e}, \ldots , \sum_{e: e \in S} c_{m,e}  \biggr) \biggr \|_{k}.
\end{align*}
%
By multiplying both sides with the denominator of the left-hand side, we get
%
\begin{align*}
    \sum_{e \in S} \sum_{i = 1}^{m} c_{i,e} \cdot & \left( \sum_{e' = 1}^{n} c_{i,e'} x_{i,e'} \right)^{k-1} \leq \\
    & \left[ \sum_{i'=1}^{m} \biggl( \biggl( \sum_{e'= 1}^{n} c_{i',e'} x_{i',e'} \biggr)^{k-1} \biggr)^{k/(k-1)} \right]^{(k-1)/k}
    	\cdot \biggl \| \biggl( \sum_{e: e \in S} c_{1,e}, \ldots , \sum_{e: e \in S} c_{m,e}  \biggr) \biggr \|_{k}
\end{align*}
which is equivalent to
\begin{align*}
    \sum_{i=1}^{m} \biggl( & \sum_{e: e \in S} c_{i,e} \biggr) \cdot \biggl( \sum_{e'=1}^{n} c_{i,e'} x_{i,e'} \biggr)^{k-1} \leq \\
    & \left \| \biggl( \sum_{e'=1}^{n} c_{1,e'} x_{1,e'} \biggr)^{k-1}, \ldots, \biggl( \sum_{e'=1}^{n} c_{m,e'} x_{m,e'} \biggr)^{k-1} \right \|_{\frac{k}{k-1}}
    		\cdot \biggl \| \biggl( \sum_{e: e \in S} c_{1,e}, \ldots , \sum_{e: e \in S} c_{m,e}  \biggr) \biggr \|_{k}
\end{align*}
The last inequality above is true by the H\"older inequality ($\| a \cdot b\|_{1} \leq \| a \|_{p} \cdot \| b \|_{q}$ where $1/p + 1/q = 1$), so the lemma holds.
\end{proof}

\begin{proposition}	\label{prop:norm}
Algorithm~\ref{algo:covering} gives an
$O(\frac{1}{1 - \eta})$-consistent and $O\bigl( k \cdot \log (d/\eta)\bigr)$-robust fractional solution
for the problem of minimizing $\|C \vect{x}\|_{k}$ under covering constraints.
\end{proposition}

\cref{prop:norm} follows immediately from our framework.
This result implies that in the standard online setting (which does not have predictions, so $\eta = 1$), Algorithm~\ref{algo:covering}
achieves the asymptotically \emph{optimal} competitive ratio of  $O\bigl( k \cdot \log (d)\bigr)$
for the problem of minimizing $\|C \vect{x}\|_{k}$ under covering constraints. (See the lower bound
of $\Omega\bigl( k \cdot \log (d)\bigr)$ in \cite{AzarCohen14:Online-Covering}.)

\subsection{Online mixed packing and covering}
In an online mixed packing and covering problem (studied in \cite{AzarBhaskar13:Online-mixed}),
we are given a set of packing constraints $C \vect{x} \leq \one$ (where $C \in \mathbb{R}^{m \times n}$),
and a set of covering constraints $B \vect{x} \geq \one$. The coefficients of all the constraints are positive,
and the packing constraints are known in advance, but the covering constraints arrive one at a time.
The goal is to increase $\vect{x} \geq \mathbb{0}$ after the arrival of each covering constraint
such that the new constraint is satisfied and the factor~$\xi$ by which $C \vect{x} \leq \xi \cdot \one$ is as small as possible. In other words, we want to satisfy the covering constraints that arrive online with the smallest possible packing constraint violation.

This problem can be formalized as: $\min \| C \vect{x}\|_{\infty}$ subject to online constraints $B \vect{x} \geq \one$.
As the $\ell_{\log m}$-norm is a constant approximation for the $\ell_{\infty}$-norm, we can replace the objective with $\min \| C \vect{x}\|_{\log m}$.
As a corollary of Proposition~\ref{prop:norm}, we get a fractional solution that is
$O(\frac{1}{1 - \eta})$-consistent and $O\bigl( \log k \cdot \log (d/\eta)\bigr)$-robust.
Moreover, in the standard online setting, our algorithm is $O\bigl( \log m \cdot \log d \bigr)$-competitive. This is \emph{optimal} up to a constant factor
due to the lower bound of $O\bigl( \log m \cdot \log d \bigr)$ in \cite{AzarBhaskar13:Online-mixed}.
