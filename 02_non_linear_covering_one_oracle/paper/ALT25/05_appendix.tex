%!TEX root = ./main.tex

\section{An example from the introduction} \label{apix:example-introduction}

The following example demonstrates why an approach where we alternate between the solutions of the prediction oracle and the primal-dual method does not work for non-linear objective functions.

Consider the objective of $x_{11}^p + ... + x_{1k}^p + x_{21}^p + ... + x_{2k}^p$ where $k$ and $p$ are some integers, and two algorithms: Algorithm $1$ and Algorithm $2$. At every time $t$, the constraint is $x_{1t} + x_{2t} \geq 1$ and Algorithm $1$ always sets $x_{1t} = 1$ and Algorithm $2$ always sets $x_{2t} = 1$. Any alternating strategy has a cost of $1$ at any time $t$. The optimal cost is $1/2^p$ at any time $t$ by setting $x_{1t} = x_{2t} = 1/2$. The competitive ratio is $2^p$. We can generalize this example for $n$ algorithms ($n$ types of variables $x_{1t}, x_{2t}, ..., x_{nt}$). The same computation gives the competitive ratio of $n^p$.

This example shows that the competitive ratio (of the algorithm that alternates between two solutions) is not captured by any function intrinsically depending only on $\lambda$ and $\mu$ (as there is a parameter $n$ in the competitive ratio). In this particular example, for a degree-$p$ polynomial with positive coefficients, $\lambda$ and $\mu$ are finite bounded values independent of $n$ (precise values can be found in Section~\ref{sec:energy}).

\clearpage

\section{Complete proof of Lemma~\ref{lem:bound-x} in \cref{sec:covering}} \label{apix:lemma-proof}
\setcounter{theorem}{0}
\begin{lemma}{BoundX}
	Let $e$ be an arbitrary resource.
	At any moment $\tau$ during the execution of the algorithm,
	when $t$ constraints have already been released, it always holds that
	$$
	x_{e}	\geq  \frac{\eta}{b^{t_{e}^{*}}_{e}(A) \ d}
			\left[ \exp\biggl( \frac{\ln(1+2d^{2}/\eta)}{\beta_{e}}
					\cdot \sum_{A: e \notin A} \sum_{t' \le t} b^{t'}_{e}(A) \cdot \alpha^{t'}_{A} \biggr) - 1 \right]
	$$
	where $b^{t_{e}^{*}}_{e}(A)$ is defined in the algorithm on line~\ref{algo-covering:bmax}.
\end{lemma}
\begin{proof}
	Let us fix a resource $e$ and prove the lemma by induction. At the beginning of the execution, when no constraint has been released yet, both sides of the lemma are 0.
	Let us assume that the lemma holds until the release of the $t^{\text{th}}$ constraint $\sum_{e} a^{t}_{e} x_{e} \geq 1$.
	Consider a moment $\tau$ during the algorithm's execution
	and let $A^{*}$ be the current set of resources $e'$ such that $x_{e'} = 1$.
	If at time~$\tau$, $x_{e} = 1$, then by the algorithm's design, the set $A^{*}$ has been updated such that
	$e \in A^{*}$. As such, the increasing rates of both sides in the lemma inequality are 0.
	In the remaining of the proof, we assume that  $x_{e} < 1$.
	We recall that by the algorithm's design, $\beta_{e} \geq \frac{1}{\lambda} \nabla_{e} F(\vect{x})$.
	We consider two cases $\beta_{e} > \frac{1}{\lambda} \nabla_{e} F(\vect{x})$
	and $\beta_{e} = \frac{1}{\lambda} \nabla_{e} F(\vect{x})$.

	\textbf{Case 1: $\beta_{e} > \frac{1}{\lambda} \nabla_{e} F(\vect{x})$.}
	In this case, by the algorithm's design, the value of $\beta_{e}$ remains unchanged at time~$\tau$ (line \ref{algo-covering:beta}) ($\frac{\partial \beta_{e}}{\partial \tau} = 0$).
	The lemma's right-hand side's derivative according to $\tau$ is
	\begin{align*}
	&\sum_{t' \le t} \frac{\partial \alpha^{t'}_{A^{*}}}{\partial \tau} \cdot
		\frac{b^{t'}_{e}(A^{*}) \ \eta }{b^{t_{e}^{*}}_{e}(A) \ d} \cdot \frac{\ln(1+2d^{2}/\eta)}{\beta_{e}}
			\cdot \exp\biggl( \frac{\ln(1+2d^{2}/\eta)}{\beta_{e} } \cdot \sum_{A: e \notin A} \sum_{t' \le t} b^{t'}_{e}(A)\ \alpha^{t'}_{A} \biggr) \\
	%
	&\leq \frac{\partial \alpha^{t}_{A^{*}}}{\partial \tau} \cdot
		\frac{b^{t}_{e}(A^{*}) \ \eta }{b^{t_{e}^{*}}_{e}(A) \ d} \cdot \frac{\ln(1+2d^{2}/\eta)}{\beta_{e}} \cdot \left( \frac{b^{t_{e}^{*}}_{e}(A) \ d}{\eta}\ x_{e} + 1 \right) \\
	%
	&= \frac{1}{\lambda \ln(1+2d^{2}/\eta)} \cdot
		\frac{b^{t}_{e}(A^{*}) \ \eta }{b^{t_{e}^{*}}_{e}(A) \ d} \cdot \frac{\ln(1+2d^{2}/\eta)}{\beta_{e}} \cdot \left( \frac{b^{t_{e}^{*}}_{e}(A) \ d}{\eta}\ x_{e} + 1 \right) \\
	%
	&\leq  \frac{b^{t}_{e}(A^{*}) \ x_{e}}{\lambda\ \beta_{e}} + \frac{\eta}{\lambda\ \beta_{e}\ d} \\
	%
	&\leq \frac{\partial x_{e}}{\partial \tau}
	\end{align*}
	%
	In the first inequality, we use the induction hypothesis and $\frac{\partial \alpha^{t}_{A^{*}}}{\partial \tau} > 0$
	and $\frac{\partial \alpha^{t'}_{A^{*}}}{\partial \tau} \leq 0$ for $t' < t$ and $\frac{\partial \beta_{e}}{\partial \tau} = 0$.
	The equality follows the increasing rate of $\alpha^{t}_{A^{*}}$.
	The last inequality is due to the increasing rate of $x_{e}$.
	The rate on the left-hand side is always larger than on the right-hand side, so the lemma inequality holds.

	\textbf{Case 2: $\beta_{e} = \frac{1}{\lambda} \nabla_{e} F(\vect{x})$.}
	In this case, by the algorithm's design, $\frac{1}{\lambda} \nabla_{e} F(\vect{x})$ is locally non-decreasing at $\tau$ (since otherwise,
	by line \ref{algo-covering:beta}, $\beta_{e}$ is not maintained to be equal to $\frac{1}{\lambda} \nabla_{e} F(\vect{x})$).
	Therefore, $\frac{\partial \beta_{e}}{\partial \tau} \geq 0$ and so $\partial \bigl(\frac{1}{\beta_{e}}\bigr)/\partial \tau \leq 0$.
	Hence, the derivative of the right-hand side of the lemma inequality according to $\tau$ is upper bounded by
	\begin{align*}
	\sum_{t' \le t} \frac{\partial \alpha^{t}_{A^{*}}}{\partial \tau} \cdot
		\frac{b^{t'}_{e}(A^{*}) \ \eta}{b^{t_{e}^{*}}_{e}(A) \ d} \cdot \frac{\ln(1+2d^{2}/\eta)}{\beta_{e}}
			\cdot \exp\biggl( \frac{\ln(1+2d^{2}/\eta)}{\beta_{e} } \cdot \sum_{A: e \notin A} \sum_{t' \le t} b^{t'}_{e}(A)\ \alpha^{t'}_{A} \biggr)
	\end{align*}
	which is bounded by $\frac{\partial x_{e}}{\partial \tau}$ by the same argument as the previous case. The lemma follows.
\end{proof}


\clearpage

\section{Complete proofs from application related
propositions} \label{sec:appix-proofs}
\setcounter{theorem}{0}

\subsection{Load Balancing}

\begin{proposition}
Algorithm~\ref{algo:covering} gives a
$O(\frac{1}{1 - \eta})$-consistent and $O\bigl((\log m) \log^{2} \frac{m}{\eta}\bigr)$-robust fractional solution
for the load balancing problem.
\end{proposition}
%
\begin{proof}
It is known that $\infty$-norm of a $m$-dim vector can be approximated by the $(\log m)$-norm,
in particular for $m \geq 2$,
$$
\|(\ell_{1}, \ell_{2}, \ldots, \ell_{m})\|_{\infty} \leq \|(\ell_{1}, \ell_{2}, \ldots, \ell_{m})\|_{\log m}
\leq m^{1/m} \|(\ell_{1}, \ell_{2}, \ldots, \ell_{m})\|_{\infty}
\leq 2 \|(\ell_{1}, \ell_{2}, \ldots, \ell_{m})\|_{\infty}.
$$
Hence, one can instead consider the objective of minimizing the  $(\log m)$-norm of the load vectors
while losing a constant factor of 2. More precisely, we consider the $(\log m)$-th power of the $(\log m)$-norm as the objective.
$$
\min \sum_{i=1}^{m} \biggl(\sum_{j} p_{ij} x_{ij}\biggr)^{\log m}
\qquad \text{s.t.} \qquad
\sum_{i=1}^{m} x_{ij} = 1 ~ \forall j
$$
%
The objective function is a polynomial of degree $\log m$. So its multilinear extension is \linebreak
$(O(k \ln(d/\eta))^{k-1}, \frac{k-1}{k \ln(1 + 2d^{2}/\eta)})$-locally smooth
with $k = \log m$ and $d = m$ (the maximal number of positive coefficients in a constraint).
Therefore, applying Theorem~\ref{thm:covering-formal}, the robustness (w.r.t the objective as  the $(\log m)$-th power of the $(\log m)$-norm)
is $O\bigl((\log m \log \frac{m}{\eta})^{\log m}\bigr)$.
Getting back to the $(\log m)$-norm objective by taking the $(\log m)$-root,
the robustness is  $O\bigl((\log m) \log^{2} \frac{m}{\eta}\bigr)$.
Hence, Algorithm~\ref{algo:covering} is $O(\frac{1}{1 - \eta})$-consistent and $O\bigl((\log m) \log^{2} \frac{m}{\eta}\bigr)$-robust.
\end{proof}

\subsection{Energy Minimization in Scheduling}

\begin{proposition}
Algorithm~\ref{algo:covering} gives a
$O(\frac{1}{1 - \eta})$-consistent and $O\bigl(k^{k} \log^{k} \frac{m}{\eta}\bigr)$-robust fractional solution
for the energy minimization problem.
\end{proposition}
%
\begin{proof}
The objective function $\sum_{i} \sum_{t} P(\sum_{j} s_{ij}(t))$ is a polynomial of degree $k = \max_{i} k_{i}$;
so its multilinear extension is
$(O(k \ln(m/\eta))^{k-1}, \frac{k-1}{k \ln(1 + 2m^{2}/\eta)})$-locally smooth
(the maximal number of positive coefficients in a constraint $d = m$).
Therefore, applying Theorem~\ref{thm:covering-formal},
Algorithm~\ref{algo:covering} provides a $O(\frac{1}{1 - \eta})$-consistent and $O\bigl(k^{k} \ln^{k} \frac{m}{\eta}\bigr)$-robust
fractional solution.
\end{proof}


\subsection{Online Submodular Mimimization}	\label{apix:sub-min}


\begin{proposition}
Algorithm~\ref{algo:covering} gives a
$O(\frac{1}{1 - \eta})$-consistent and $O\bigl( \frac{\log (d/\eta)}{1 - \kappa_{f}} \bigr)$-robust fractional  solution
for the submodular minimization under covering constraints.
\end{proposition}
\begin{proof}
Let $F$ be the multilinear extension of $f$.
It is sufficient to verify that $F$ is $\bigl(\frac{1}{1-\kappa_{f}},0\bigr)$-locally smooth.
Recall that, by definition of the multilinear extension,
$F(\vect{x}) = \mathbb{E} \bigl[ f(\one_{T})\bigr]$ where $T$ is a random set
such that a resource $e$ appears in $T$ with probability $x_{e}$. Moreover, as $F$ is linear in $x_{e}$, we have
%
\begin{align*}
\nabla_{e} F(\vect{x}) %= \frac{\partial F(\vect{x}) }{\partial x_{e}}
&= F(x_{1}, \ldots, x_{e-1}, 1, x_{e+1}, \ldots, x_{n}) - F(x_{1}, \ldots, x_{e-1}, 0, x_{e+1}, \ldots, x_{n}) \\
&= \mathbb{E} \biggl[ f\bigl(\one_{R \cup \{e\}}\bigr) - f\bigl(\one_{R}\bigr) \biggr]
\end{align*}
where $R$ is a random subset of resources $N \setminus \{e\}$ such that $e'$ is included with probability $x_{e'}$.
Therefore, to prove that $F$ is $(\lambda,\mu)$-locally-smooth, it is equivalent to show that,
for any set $S \subset \mathcal{E}$ and for any vectors $\vect{x}^{e} \in [0,1]^{n}$ for $e \in \mathcal{E}$,
%
\begin{equation*}	\label{eq:min-local-smooth-equiv}
\sum_{e \in S} \mathbb{E} \biggl[ f\bigl(\one_{R^{e} \cup \{e\}}\bigr) - f\bigl(\one_{R^{e}}\bigr) \biggr]
\leq \lambda f\bigl( \one_{S} \bigr) + \mu \mathbb{E} \biggl[ f\bigl(\one_{R}\bigr) \biggr]
\end{equation*}
%
where $R^{e}$ is a random subset of resources $N \setminus \{e\}$ such that $e'$ is included with probability $x^{e}_{e'}$
and $R$ is a random subset of resources $N \setminus \{e\}$ such that $e'$ is included with probability $\max_{e \in S} x^{e}_{e'}$.

The $\bigl(\frac{1}{1-\kappa_{f}},0\bigr)$-local smoothness of $F$ holds due to submodularity and Lemma~\ref{lem:curvature} (see below), so
for any subsets $R^{e}$, we have
\begin{align*}
	\sum_{e \in S} \left[ f\bigl(\one_{R^{e} \cup \{e\}}\bigr) - f\bigl(\one_{R^{e}}\bigr) \right]
		\leq \sum_{e \in S} \left[ f\bigl(\one_{\{e\}}\bigr) \right]
		\leq \frac{1}{1 -\kappa_{f}} \cdot f(\one_{S})
\end{align*}
Therefore, applying Theorem~\ref{thm:covering-formal}, the proposition follows.
%Algorithm~\ref{algo:covering} gives a fractional
%$O(\frac{1}{1 - \eta})$-consistent and $O\bigl( \frac{\log (d/\eta)}{1 - \kappa_{f}} \bigr)$-robust solution.
\end{proof}

\begin{lemma}		\label{lem:curvature}
	For any set $S$, it always holds that
	$$
	f(\one_{S}) \geq (1-\kappa_{f}) \sum_{e \in S} f(\one_{\{e\}}).
	$$
\end{lemma}
\begin{proof}
	Let $S = \{e_{1}, \ldots, e_{m}\}$ be an
	arbitrary subset of $\mathcal{E}$. Let $S_{i} = \{e_{1}, \ldots, e_{i}\}$ for $1 \leq i \leq m$ and $S_{0} = \emptyset$.
	We have
	\begin{align*}
	f(\one_{S})
	&\geq  f(\one_{\mathcal{E}}) -  f(\one_{\mathcal{E} \setminus S})
	= \sum_{i=0}^{m-1}  f(\one_{\mathcal{E} \setminus S_{i}}) - f(\one_{\mathcal{E} \setminus S_{i+1}})
	\geq \sum_{i=1}^{m}  f(\one_{\mathcal{E}}) - f(\one_{\mathcal{E} \setminus \{e_{i}\}}) \\
	&\geq (1 - \kappa_{f}) \sum_{i=1}^{m} f(\one_{e_{i}})
	\end{align*}
	where the first two inequalities are due to the submodularity of $f$, and the last inequality follows the definition of curvature.
\end{proof}
