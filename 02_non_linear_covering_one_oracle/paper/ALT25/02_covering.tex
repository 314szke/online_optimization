%!TEX root = ./main.tex

\section{The framework}		\label{sec:covering}

\subsection{Formulation}
We formulate the online covering problem that we described in the Preliminaries as a problem of finding the minimum cost solution among all the possible solutions. This formulation has an exponential number of variables and constraints; however, it allows us to transform the non-linear objective function into a linear one, which is crucial for our algorithm and proofs.

Let $S \subseteq \mathcal{E}$ be a \emph{solution} if $\one_{S}$ corresponds to a feasible solution. Let $x_{e}$ be a variable indicating whether resource $e$ is selected.
Let $z_{S}$ be an indicator variable for solution $S$. If $z_{S} = 1$, then every variable
$x_{e} = 1$ if $e \in S$, and $x_{e} = 0$ if $e \notin S$. Otherwise, $z_S = 0$. In other words, $z_{S} = 1$ if and only if $\one_{S}$ is the selected solution of the online covering problem. At each time step~$t$ during the execution, a new constraint is revealed. For every subset $A \subseteq \mathcal{E}$, we define the value $c^{t}(A) := \max\{0,\ 1 - \sum_{e \in A} a^{t}_{e}\}$, to be the amount we need until constraint satisfaction. Given this value, we normalize the constraint coefficients to be $a^{t}_{e}(A) := \min\{a_{e}^{t},\ c^{t}(A)\}$. Finally, we define $b^{t}_{e}(A) := a^{t}_{e}(A)\ /\ c^{t}(A)$ where $c^{t}(A) > 0$. The values $b^{t}_{e}(A)$ correspond to the coefficients in the knapsack inequality constraints \citep{CarrFleischer:2000}. The primal and dual programs are:

\vspace{-0.6cm}
\begin{minipage}[t]{0.45\textwidth}
	\begin{align*}
		\min  \sum_{S \subseteq \mathcal{E}} &f(\one_{S})\ z_{S} \\
		\sum_{e \notin A} b_{e}^{t}(A) \ x_{e} &\geq 1 & &  \forall t,\ \forall A \subseteq \mathcal{E} \\
		\sum_{S: e \in S} z_{S}  &= x_{e}	& & \forall e \\
		\sum_{S \subseteq \mathcal{E}} z_{S} &= 1 & & \\
		x_{e}, z_{S} &\in \{0,1\} & & \forall e,\ \forall S \subseteq \mathcal{E}\\
	\end{align*}
\end{minipage}
\quad
\begin{minipage}[t]{0.5\textwidth}
	\begin{align*}
		\max \sum_{t, A} \alpha^{t}_{A} &+ \gamma \\
		\sum_{t} \sum_{A: e \notin A} b_{e}^{t}(A) \ \alpha_{A}^{t} &\leq \beta_{e}  & &  \forall e \\
		\gamma + \sum_{e \in S} \beta_{e} &\leq f(\one_{S})  & & \forall S \subseteq \mathcal{E}\\
		\alpha^{t}_{A} &\geq 0 & & \forall t,\ \forall A \subseteq \mathcal{E}\\
		\beta_e &\geq 0 & & \forall e\\
		\gamma &\geq 0 & &
	\end{align*}
\end{minipage}
\vspace{-0.6cm}

In the primal program, the first constraints are knapsack-constraints \citep{CarrFleischer:2000} of the given polytope, and they are equivalent to $\sum_{e \notin A} a_{e}^{t}(A) \ x_{e} \geq c^{t}(A)$. It is sufficient to satisfy constraints where $c^{t}(A) > 0$. The second primal constrain ensures that if a resource $e$ is chosen, the selected solution must contain $e$.
The third constraint guarantees that \emph{one} solution is selected.

\subsection{Algorithm}
In our proposed algorithm, $\vect{x} \in [0, 1]^{|\mathcal{E}|}$ corresponds to the current solution of the algorithm. During the execution, we rely on the objective function's multilinear extension $F$, parametrized by $\lambda$ and $\mu$. We assume, that $F(\vect{x})$ is $(\lambda, C \mu)$-locally-smooth, where $C$ is a constant that arises from the algorithm's analysis (see Lemma~\ref{lem:prim-dual-feasible}). Algorithm~\ref{algo:covering} follows the scheme of \cite{Thang20:Online-Primal-Dual}, which uses both the primal and dual variables to solve the problem.

We have two notions of time in our algorithm. First, at each discrete time step $t$, a new primal constraint arrives. Second, we have a continuous time $\tau$ throughout the execution. The solution of the algorithm increases gradually with time $\tau$.  To simplify the notations, when the context only uses the current time of the execution, $\vect{x}$ refers to $\vect{x}(\tau)$, the current solution at time $\tau$.

\begin{algorithm}[!ht]
	\begin{algorithmic}[1]
	\STATE Initially, set $A^* \gets \emptyset$ \ \ \texttt{(where $A^*$ is the solution set and $\forall e \in A^* : x_{e} = 1$)}
	\STATE All primal and dual variables are initially set to 0
	\STATE During every step, for each feasible solution $S$, $z_{S} = \prod_{e \in S} x_{e} \prod_{e \notin S} (1 - x_{e})$ is maintained.
	\STATE Let $\tau$ be the continuous timer during the execution of the algorithm.
	\FOR{each time $t$, for the new primal constraint $\sum_{e} a_{e}^{t} x_{e} \geq 1$
	and dual variable $\alpha^{t}_{A^*}$}
		\WHILE[\texttt{Increase primal, dual variables}]{$\sum_{e \notin A^{*}} b^{t}_{e}(A^{*})\ x_{e} < 1$}
			\STATE Increase $\tau$ with a rate of $1$.
			\STATE Increase $\alpha^{t}_{A^{*}}$ at rate $1\ /\ (\lambda \ \ln(1+2d^{2}/\eta))$
			%(Note that $c_{k,A^{*}} > 0$ by the condition of the while loop.)
			\label{algo-covering:alpha}
			\FOR{$e \notin A^{*}$ such that $b^{t}_{e}(A^{*}) > 0$}
				\STATE \textbf{if} $\beta_{e} <  \frac{1}{\lambda} \nabla_{e} F(\vect{x})$ \textbf{then}
				$\beta_{e} \gets \frac{1}{\lambda} \nabla_{e} F(\vect{x})$
				\label{algo-covering:beta}
				\STATE Increase $x_{e}$ at a rate according to the following
				\begin{align*}
					\frac{\partial x_{e}}{\partial \tau}	\gets
					\frac{b^{t}_{e}(A^{*}) \ x_{e}}{\lambda \beta_{e}} + \frac{\eta}{\lambda \beta_{e} d}
					+ \frac{(1 - \eta) \cdot \one_{\{pred(x_{e}) = 1\}}}{\nabla_{e} F(\vect{x}) \cdot |\{e': pred(x_{e'}) = 1,\ b^{t}_{e'}(A^{*}) > 0\}| }
				\end{align*}
				\label{algo-covering:x}
			\ENDFOR
			\STATE \textbf{if} $x_{e} = 1$ \textbf{then} $A^{*} \gets A^{*} \cup \{e\}$
			\FOR[\texttt{Decrease dual variables}]{$e : e \notin A^*$} \label{algo-decrease}
				\WHILE{
					$\sum_{t'=1}^{t} \sum_{A: e \notin A} b^{t'}_{e}(A) \ \alpha^{t'}_{A} > \beta_{e}$}
						\FOR{$(t_{e}^{*}, A) \textnormal{ such that } b^{t_{e}^{*}}_{e}(A) =  \max \{b^{t'}_{e}(A)\ |\ \forall A: e \notin A \textnormal{ and }\forall t' \leq t \textnormal{ s.t. } \alpha^{t'}_{A} > 0\}$} \label{algo-covering:bmax}
							\STATE Decrease $\alpha^{t_{e}^{*}}_{A}$ continuously with a rate of
							$\frac{b^{t}_{e}(A^{*})}{b^{t_{e}^{*}}_{e}(A)} \cdot\frac{1}{\lambda \cdot \ln(1+2d^{2}/\eta)}$
							\label{algo-covering:decrease}
					\ENDFOR
				\ENDWHILE
			\ENDFOR
		\ENDWHILE
	\ENDFOR
	\end{algorithmic}
	\caption{Online Algorithm for Non-Linear Covering Problems.}
	\label{algo:covering}
\end{algorithm}

When a new primal constraint arrives, the current dual variable $\alpha^{t}_{A^*}$ increases at a constant rate (line \ref{algo-covering:alpha}), while the $\beta_e$ variables are updated according to the partial derivative of the mulitlinear extension (line \ref{algo-covering:beta}). We note a subtle point here: if $\beta_e < \frac{1}{\lambda} \nabla_{e} F(\vect{x})$ then we set
$\beta_e = \frac{1}{\lambda} \nabla_{e} F(\vect{x})$, but if $\beta_e > \frac{1}{\lambda} \nabla_{e} F(\vect{x})$ then we do not change the value of $\beta_e$. This update preserves the following invariants during the execution of the algorithm: $\beta_{e} \geq \frac{1}{\lambda} \nabla_{e} F(\vect{x})$ and $\beta_{e}$ is non-decreasing. (Remark: if $\nabla_{e} F(\vect{x})$ is monotone on every coordinate $e$, then it is sufficient to always set $\beta_{e} \gets \frac{1}{\lambda} \nabla_{e} F(\vect{x})$.)

The update on line \ref{algo-covering:x} is inspired by the multiplicative weight update method (where the increasing rate of $x_{e}$
is inversely proportional to $\beta_{e}$) and the updating approach of \cite{BamasMaggiori20:The-Primal-Dual-method}. The oracle's prediction skews the weights to assign more value to the predicted coordinates $x$.
Starting from line \ref{algo-decrease}, the algorithm decreases some of the dual variables using a similar idea as in
\cite{AzarBuchbinder16:Online-Algorithms}. This decrease is necessary to maintain the feasibility of the dual solution.

\subsection{Primal and dual variables}
Let $\vect{x}(\tau)$ be the algorithm's primal solution at time $\tau$. The dual variables $\alpha^{t}_{A}$ and $\beta_{e}$ are assigned during the execution, but not $\gamma$. To make the dual solution feasible, we set $\gamma = -\frac{\mu}{4\lambda \cdot \ln(1+2d^{2}/\eta)} F(\vect{x}(\tau))$ (see Lemma~\ref{lem:prim-dual-feasible}). Each $\beta_{e} = \frac{1}{\lambda} \nabla_{e} F(\vect{x}(\tau'))$, for some primal solution $\vect{x}(\tau')$, where $\tau' \le \tau$. Moreover, $\vect{x}(\tau) \geq \bigvee_{\tau' \le \tau} \vect{x}(\tau')$ (each coordinate $x_{e}(\tau) = \max_{\tau' \le \tau}\{x_{e}(\tau')\}$), since the $x_e$-variables are non-decreasing. Consequently, each $\beta_{e} \geq \frac{1}{\lambda} \nabla_{e} F(\vect{x}(\tau))$. Using these properties, Lemma~\ref{lem:bound-x} gives a lower bound on $\vect{x}(\tau)$. We highlight that the proof if this lemma does \emph{not} require the gradient of $F$ to be monotone.
(With this assumption, the algorithm can set $\beta_{e} = \frac{1}{\lambda} \nabla_{e} F(\vect{x}(\tau))$ at each step $\tau$ of the execution.)

\setcounter{theorem}{0}
\begin{lemma}
\label{lem:bound-x}
Let $e$ be an arbitrary resource.
At any moment $\tau$ during the execution of the algorithm,
when $t$ constraints have already been released, it always holds that
$$
x_{e}	\geq  \frac{\eta}{b^{t_{e}^{*}}_{e}(A) \ d}
		\left[ \exp\biggl( \frac{\ln(1+2d^{2}/\eta)}{\beta_{e}}
				\cdot \sum_{A: e \notin A} \sum_{t' \le t} b^{t'}_{e}(A) \cdot \alpha^{t'}_{A} \biggr) - 1 \right]
$$
where $b^{t_{e}^{*}}_{e}(A)$ is defined in the algorithm on line~\ref{algo-covering:bmax}.
\end{lemma}
\begin{proof}
	The proof is available in \cref{apix:lemma-proof}.
\end{proof}
%
\begin{lemma} \label{lem:prim-dual-feasible}
The primal and dual variables are feasible.
\end{lemma}
%
\begin{proof}

\textbf{Primal feasibility.}
While a primal covering constraint is unsatisfied, the $x_e$-variables are increasing. At the end of the first iteration, the first primal covering constraint is satisfied. Afterwards, the new constraints are
also satisfied, since the algorithms maintains $z_{S} = \prod_{e \in S} x_{e} \prod_{e \notin S} (1 - x_{e})$.
If we choose an element $e$ with probability $x_{e}$, then $z_{S}$ is the probability
that the set of selected items is $S$. Therefore, the total probability $\sum_{S} z_{S} = 1$. By a similar argument, we get the following:
\[
	\sum_{S: e \in S} z_{S} = x_{e} \sum_{S' \subseteq E \setminus \{e\}} \prod_{e' \in S'} x_{e'} \prod_{e' \notin S'} (1 - x_{e'}) = x_{e}
\]
since
\[
	\sum_{S' \subseteq E \setminus \{e\}} \prod_{e' \in S'} x_{e'} \prod_{e' \notin S'} (1 - x_{e'}) = 1
\].

\textbf{Dual feasibility.} Let us now prove that the first dual constraint is always satisfied during the execution. The algorithm maintains $\sum_{t' \le t} \sum_{A: e \notin A} b^{t'}_{e}(A)\ \alpha^{t'}_{A} \leq \beta_{e}$. % (strict inequality happens only if $x_{e} = 1$).
Whenever this inequality is violated, the algorithm decreases (see line \ref{algo-covering:decrease}) some of the $\alpha$-variables in a way that the increasing rate of $\sum_{t' \le t} \sum_{A: e \notin A} b^{t'}_{e}(A)\ \alpha^{t'}_{A}$ is at most 0. By the $\beta$-variables' definition, the first dual constraint holds.

Let us consider the second dual constraint. Let $\vect{x}(\tau)$ be the final solution of the algorithm. For each fixed resource $e$, the value $\beta_{e} = \frac{1}{\lambda} \nabla_{e} F(\vect{x}(\tau_e))$ for some previous solution $\vect{x}(\tau_e)$ where $\tau_e \le \tau$ and where $x_{e}(\tau_e) \leq x_{e}(\tau)$ for all $e$.
Let $\vect{y} := \bigvee_{\tau' \le \tau} \vect{x}(\tau') \leq \vect{x}(\tau)$, so for each coordinate $e$ of $\vect{y}$, we have $y_{e} = \max_{\tau' \le \tau}\{x_{e}(\tau')\}$.
By definition of the dual variables, the second dual constraint (after rearranging the terms) reads
\begin{align*}
	\frac{1}{\lambda} \sum_{e \in S} \nabla_{e} F(\vect{x}(\tau_e)) \leq F(\one_{S}) + \frac{\mu}{4\lambda \cdot \ln(1+2d^{2}/\eta)} F(\vect{x}(\tau)) \quad \quad \forall\ S \subseteq \mathcal{E}
\end{align*}
since we set $\gamma = -\frac{\mu}{4\lambda \cdot \ln(1+2d^{2}/\eta)} F(\vect{x}(\tau))$, and $\vect{x}(\tau_e)$ corresponds to the solution during the execution where $\beta_e$ was set to $\frac{1}{\lambda} \nabla_{e} F(\vect{x}(\tau_e))$.
Since $F$ is monotone, $F(\vect{x}(\tau)) \geq F(\vect{y})$. To prove that the above inequality holds, it is sufficient to show that
\begin{align*}
	\frac{1}{\lambda} \sum_{e \in S} \nabla_{e} F(\vect{x}(\tau_e)) \leq F(\one_{S}) + \frac{\mu}{4\lambda \cdot \ln(1+2d^{2}/\eta)} F(\vect{y})
\end{align*}
which means that $F$ needs to be  $\bigl(\lambda, \frac{\mu}{4\ln(1+2d^{2}/\eta)}\bigr)$-locally-smooth. Our initial assumption was that $F$ is $(\lambda, C \mu)$-locally-smooth. By setting $C := \frac{1}{4\ln(1+2d^{2}/\eta)}$, the lemma holds.
\end{proof}

\setcounter{theorem}{0}
\begin{theorem} \label{thm:covering-formal}
	Let $F$ be the multilinear extension of the online non-linear covering problem's objective function $f$ and
	$d$ be the maximal row sparsity of the constraint matrix (formally $d = \max_{t \le T} |\{a^{t}_{e}: a^{t}_{e} > 0\}|$).
	Assuming that $F$ is $\bigl(\lambda, \frac{\mu}{\ln(1+2d^{2}/\eta)}\bigr)$-locally-smooth
	for some parameters ($\lambda > 0$, $\mu < 1$ and $0 < \eta \leq 1$), there exists an $O\bigl( \frac{1}{1 - \eta} \bigr)$-consistent and $O\bigl( \frac{\lambda}{1 - \mu}  \cdot \ln \frac{d}{\eta} \bigr)$-robust algorithm for any $\eta \in (0,1]$ which produces a fractional solution for the given problem.
\end{theorem}
\begin{proof}

\textbf{Robustness.}
By bounding the increases of the primal and dual objective values at any time $\tau$ during the execution of
Algorithm~\ref{algo:covering}, we can determine the robustness. Upon the release of the $t^{\text{th}}$ constraint,
let $A^{*}$ be the current solution with the chosen set of resources $e$ such that $x_{e} = 1$.

\noindent The derivative of the primal objective function with respect to $\tau$ is:
%
\begin{align}	\label{eq:covering-primal}
\sum_{e \in \mathcal{E}} & \nabla_{e} F(\vect{x}) \cdot \frac{\partial x_{e}}{ \partial \tau} \notag \\
&= \sum_{e\ :\ b^{t}_{e}(A^{*})\ >\ 0} \nabla_{e} F(\vect{x})
	\ \left( \frac{b^{t}_{e}(A^{*}) \ x_{e}}{\lambda\ \beta_{e}} + \frac{\eta}{\lambda\ \beta_{e}\ d}
		 + \frac{(1 - \eta) \ \one_{\{pred(x_{e})\ =\ 1\}}}{\nabla_{e} F(\vect{x}) \cdot |\{e': pred(x_{e'})\ =\ 1,\ b^{t}_{e'}(A^{*}) > 0\}| }
 \right)	\notag \\
%
&\leq  \sum_{e\ :\ b^{t}_{e}(A^{*})\ >\ 0} \biggl( b^{t}_{e}(A^{*}) \ x_{e} + \frac{\eta}{d} \biggr)
	+ \sum_{e\ :\ pred(x_e)\ =\ 1 \atop b^{t}_{e}(A^{*})\ >\ 0} \frac{(1 - \eta)}{ |\{e': pred(x_{e'}) = 1,\ b^{t}_{e'}(A^{*}) > 0\}|  } \notag \\
& \leq 2
\end{align}
The first inequality follows $\nabla_{e} F(\vect{x}) \leq \lambda \  \beta_{e}$. The second inequality is
due to the definition of $d$ and  the fact that
$\sum_{e \notin A^{*}} b^{t}_{e}(A^{*}) \ x_{e} \leq 1$ always holds during the algorithm. (The number of $b^{t}_{e}(A^{*})$ values which are strictly greater than 0, is at most $d$.)

At any time $\tau$, let $U(\tau)$ be the set of resources $e$ such that
$\sum_{t' \le t} \sum_{A: e \notin A} b^{t'}_{e}(A)\ \alpha^{t'}_{A} = \beta_{e}$ and $b^{t}_{e}(A^*) > 0$.
Note that $|U(\tau)| \leq d$ by definition of $d$.
As long as $\sum_{e \notin A^{*}} b^{t}_{e}(A^*)\ x_{e}  < 1$,
using Lemma~\ref{lem:bound-x} we get that for every $e \in U(\tau)$,
%
\begin{align*}
\frac{1}{b^{t}_{e}(A^*)} > x_{e} \geq \frac{\eta}{b^{t_{e}^{*}}_{e}(A) \ d}
				\left[ \exp\biggl(\ln(1+2d^{2}/\eta) \biggr) - 1 \right]
				= \frac{2 d}{b^{t_{e}^{*}}_{e}(A)}
\end{align*}
where $b^{t_{e}^{*}}_{e}(A)$ is defined in the algorithm on line~\ref{algo-covering:bmax}.
Therefore, $\frac{b^{t}_{e}(A^{*})}{b^{t_{e}^{*}}_{e}(A)} \leq \frac{1}{2d}$. Following the definition of $U(\tau)$, we can bound the increase of the dual at time $\tau$.

\noindent The derivative of the dual with respect to $t$ is:
\begin{align*}
\frac{\partial D}{\partial \tau}
&=  \sum_{t' \le t} \sum_{A : e \notin A} \frac{\partial \alpha^{t'}_{A}}{\partial \tau} + \frac{\partial \gamma}{\partial \tau}
= \sum_{t' \le t} c^{t'}(A^{*}) \cdot \frac{\partial \alpha^{t'}_{A^{*}}}{\partial \tau} + \frac{\partial \gamma}{\partial \tau} \\
&= \frac{1}{\lambda \cdot \ln(1 + 2d^{2}/\eta)} \cdot \biggl(1 - \sum_{e \in U(\tau)} \frac{b^{t}_{e}(A^{*})}{b^{t_{e}^{*}}_{e}(A)} \biggr)
	- \frac{\mu}{4\lambda \cdot \ln(1 + 2d^{2}/\eta)} \cdot \sum_{e} \nabla_{e} F(\vect{x}) \frac{\partial x_{e}}{\partial \tau} \\
%
&\geq \frac{1}{\lambda \cdot \ln(1 + 2d^{2}/\eta)} \biggl(1 - \sum_{e \in U(\tau)} \frac{1}{2d} \biggr)
	- \frac{\mu}{2 \lambda \cdot \ln(1 + 2d^{2}/\eta)} \\
%
&\geq \frac{1 - \mu}{2 \lambda \cdot \ln(1 + 2d^{2}/\eta)}.
\end{align*}
The third equality holds since $\alpha^{t}_{A^{*}}$ is increased and other $\alpha$-variables in
$U(\tau)$ are decreased. The first inequality uses the fact that $\frac{b^{t}_{e}(A^{*})}{b^{t_{e}^{*}}_{e}(A)} \leq \frac{1}{2d}$
and Inequality~(\ref{eq:covering-primal}).
The last inequality holds since $|U(\tau)| \leq d$.
Hence, the robustness is at least $\frac{4 \lambda}{1 - \mu} \cdot \ln(1 + 2d^{2}/\eta)$.

\textbf{Consistency.} We establish consistency with a similar argument as \cite{BamasMaggiori20:The-Primal-Dual-method}.
Considering an arbitrary moment $\tau$ during the algorithm's execution, let $S_{1} = S_{1}(\tau)$ be the set of resources selected by the prediction. Formally, $\forall\ e \in S_{1} : pred(x_{e}) = 1$ up to time $\tau$. Let $S_{2} = S_{2}(\tau)$ contain the remaining resources.
The primal objective increase due to $S_{1}$ and $S_{2}$:

\begin{align*}
\sum_{e \in S_{1}} \nabla_{e} F(\vect{x})\ \frac{\partial x_{e}}{ \partial \tau}
&= \sum_{e \in S_{1}} \nabla_{e} F(\vect{x})
	\ \left( \frac{b^{t}_{e}(A^{*}) \ x_{e}}{\lambda\ \beta_{e}} + \frac{\eta}{\lambda\ \beta_{e}\ d}
		 + \frac{(1 - \eta)}{\nabla_{e} F(\vect{x}) \cdot |\{e': pred(x_{e'}) = 1\}| }
 \right)\\
 &\geq 1 - \eta \\
 %
 \sum_{e \in S_{2}} \nabla_{e} F(\vect{x})\ \frac{\partial x_{e}}{ \partial \tau}
&= \sum_{e \in S_{2}} \nabla_{e} F(\vect{x})
	\ \left( \frac{b^{t}_{e}(A^{*}) \ x_{e}}{\lambda\ \beta_{e}} + \frac{\eta}{\lambda\ \beta_{e}\ d} \right)
\leq 1 + \eta
\end{align*}
Therefore, the primal objective increase is at most $\bigl(1 +  \frac{1 + \eta}{1 - \eta} \bigr)$ time the increase restricted to
the set~$S_{1}$. Moreover, the algorithm's primal objective value restricted to $S_{1}$ is smaller than
the prediction's, since $\forall e \in S_{1} : x_{e} \leq 1 = pred(x_{e}$).
We can deduce that the algorithm is $O\bigl( \frac{1}{1 - \eta} \bigr)$-consistent with the prediction.
\end{proof}
