%!TEX root = ./main.tex

\section*{Appendix}

\section{Models}


\subsection{Comparaison between \texttt{LIN-COMB} and the best-expert in hindsight models}
In this section, we argue that (1) in the linear covering setting, \texttt{LIN-COMB} and the best-expert in hindsight benchmark are equivalent;
and (2) in the non-linear covering one,  \texttt{LIN-COMB} is stronger than the best-expert in hindsight benchmark. 

First, recall the best expert in hindsight benchmark. There are $K$ experts, and the problem's covering-type constraints are revealed online one by one.
At each time $t \geq 1$, we receive a covering constraint $\sum_{i=1}^{n} a_{i}^{t} x_{i} \geq 1$ (where $a_{i}^{t} \geq 0$) and each expert $k$ (where $1 \leq k \leq K$) provides
a solution $(s_{i,k}^{t})_{i=1}^{n}$. The objective is to minimize a (cost) function $f(x_{1}, \ldots, x_{n})$. The optimal value in the best expert benchmark, up to time $T$, is 
$$
\min_{k} f(s_{1,k}^{T}, \ldots, s_{n,k}^{T})
$$
%
Note that the best experts can be changed over time (i.e., the best expert at time $T$ might be different to the one at time $T-1$).
In the following, let $T$ be the last time a constraint is released. 

\paragraph{Linear Covering Setting.} In this setting, the objective function $f(x_{1}, \ldots, x_{n})$ can be expressed as $\sum_{i=1}^{n} c_{i} x_{i}$ where $c_{i} \geq 0$.
Let $k^{*}$ be the best expert, i.e., $k^{*} = \arg \min_{k} \sum_{i=1}^{n} c_{i} s_{i,k}^{T}$. The \texttt{LIN-COMB} can achieve this value by setting 
$w_{k^{*}}^{t} = 1$ and $w_{k}^{t} = 0$ for all $k \neq k^{*}$ over all $1 \leq t \leq T$. Inversely, assume that \texttt{LIN-COMB} benchmark provides $w_{1}^{T}, \ldots, w_{k}^{T}$
at time $T$. Then, the optimal value of \texttt{LIN-COMB} (at time $T$) is:
\begin{align*}
\sum_{i=1}^{n} c_{i} x_{i} &\geq \sum_{i=1}^{n} c_{i} \biggl( \sum_{k=1}^{K} w_{k}^{T} s_{i,k}^{T} \biggr)
= \sum_{k=1}^{K} w_{k}^{T}  \biggl( \sum_{i=1}^{n} c_{i} s_{i,k}^{T} \biggr) \\
%
&\geq \biggl ( \sum_{k=1}^{K} w_{k}^{T} \biggr) \cdot \biggl( \sum_{i=1}^{n} c_{i} s_{i,k^{*}}^{T} \biggr)
=  \sum_{i=1}^{n} c_{i} s_{i,k^{*}}^{T}.
\end{align*}

Consequently, the two benchmarks are equivalent. 

\paragraph{Non-Linear Covering Setting.} By the previous argument, \texttt{LIN-COMB} always captures the best expert benchmark by setting 
$w_{k^{*}}^{t} = 1$ and $w_{k}^{t} = 0$ for all $k \neq k^{*}$ over all $1 \leq t \leq T$. We are providing an example showing that \texttt{LIN-COMB} is strictly stronger than 
the best-expert. Consider the makespan minimization problem in which one assigns $n$ unit jobs to $n$ identical machines to minimize the maximum machine load (makespan).   
This problem can be formulated as a non-linear covering problem.
$$
\min \biggl \| \biggl( \sum_{j} x_{1j}, \sum_{j} x_{2 j}, \ldots, \sum_{j} x_{n j} \biggr) \biggr \|_{\infty}
\qquad {\text{s.t}} \qquad
\sum_{i=1}^{n} x_{ij} \geq 1 ~\forall j, \quad x_{ij} \geq 0 ~\forall i,j
$$

There are $n$ experts and each expert $i$ assigns all jobs to machine $i$ for $1 \leq i \leq n$. 

The best expert has the maximum machine load of $n$ (the same for every expert). However, the optimal solution in \texttt{LIN-COMB} can choose $w_{i} = 1/n$, which results in the makespan of $1$. The solution corresponds to the assignment of one job to one machine.  


\subsection{Assumptions of experts}
We consider two natural assumptions on the suggestions/solutions of experts. 

The first one is \emph{feasibility}, i.e., experts' solutions must satisfy all the constraints so far. One can easily check and detect if a solution is infeasible and ignore the corresponding expert if that is the case. 

The second one is the \emph{monotonicity} of the expert solution, i.e., $s_{i,k}^{t} \geq s_{i,k}^{t-1}$ for all $i,k,t$. We argue that any well-defined benchmark that includes the best expert in hindsight must have this property.

Consider a linear covering problem $Nx_{1} + x_{2}$ where $N$ is large and constraint at time $1$ is $x_{1} + x_{2} \geq 1$ and constraint at time $2$ is $2x_{1} + x_{2} \geq 1$. 
There are two experts. 
At time $1$, experts 1 and 2 provide solution $(1,0)$ and $(\infty, \infty)$ respectively. 
The best expert up to time 1 is expert 1 with an objective cost of $N$. 
At time $2$, expert 1 always provides $(1,0)$ while expert 2 gives a solution $(0,1)$ by \emph{decreasing} its previous solution.  
Now the best expert up to time 2 is expert 2 with an objective cost of $1$. 
One can generalize this toy example to create an unstable objective cost (very large then very small and so on) of the best expert benchmark.

In the above example, no algorithm based on expert solutions can be competitive 
since an adversary can create a dummy expert (providing a trivial solution with a large objective) all the time, but the last step 
and in the last step, the dummy expert gives an optimal solution. An online algorithm cannot decrease its solution (as decisions made are irrevocable)
and becomes uncompetitive. 

\section{Counter example for tight online expert solutions} \label{appix-tight-solutions}

The following example shows that we cannot expect online expert solutions (in the sense of online algorithms)
to be tight on the arriving constraints. In the example below, we display the experts' solutions after each constraint.

\begin{align*}
    \min\ \ \ \ x_{1} + \ \ \ x_{2} & \\
    x_{1} + \frac{1}{2}\ x_{2} & \ge 1 \\
    \textnormal{Expert}_{1}: \ \ \ \ \ \ \ \ \ \ \ 1 \ \ \ \ \ \ \ \ 0 & \\
    \textnormal{Expert}_{2}: \ \ \ \ \ \ \ \ \ \ \ 0 \ \ \ \ \ \ \ \ 2 & \\
    x_{2} & \ge 1 \\
    \textnormal{Expert}_{1}: \ \ \ \ \ \ \ \ \ \ \ 1 \ \ \ \ \ \ \ \ 1 & \\
    {\color{red}\textnormal{Expert}_{2}:} \ \ \ \ \ \ \ \ \ \ \ \ {\color{red} 0} \ \ \ \ \ \ \ \ {\color{red} 2} & \\
\end{align*}
%
To have tight a suggestion from Expert$_2$ on the second constraint, Expert$_2$ not only has to decrease its value of $x_{2}$ (which is not allowed), but even increase the value of $x_{1}$ for the first constraint. In other words, Expert$_2$ has to completely modify its past decisions.
