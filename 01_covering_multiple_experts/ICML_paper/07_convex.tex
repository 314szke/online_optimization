%!TEX root = ./main.tex

\section{Online \emph{convex} covering with experts} \label{sec:convex}

\subsection{Benchmark reformulation for the algorithm}

While linear programs are suitable to represent many optimization problems, they are not expressive enough to capture some practically relevant problems (for example, makespan minimization and congestion management). This section takes a step further towards a general framework and proposes an algorithm with multiple experts for $0$-$1$ optimization problems with convex objectives and linear constraints.

The performance guarantee proof of our previous algorithm relies on the primal-dual method, which bounds the algorithm's total cost by bounding at each time step the primal with the dual increase.
The \texttt{LIN-COMB} benchmark's convex objective function (visible on \cref{fig:benchmark}) does not allow us to express the objective's cost by additive terms like in the linear case. To keep the primal-dual proving technique, we reformulate the \texttt{LIN-COMB} benchmark as finding the minimum cost linear combination among all possible linear combinations. This formulation has an exponential number of variables and constraints, but it transforms the convex objective function into a linear one.

To explain the reformulation, let us first define $\mathcal{E} = \{1,2,\dots,n\}$ as a set that contains all the indices of the original covering problem's variable $x$. Then, let $S \subseteq \mathcal{E}$ be a solution, if $\vect{1}_{S}$ corresponds to a feasible solution. Recall that we consider $0$-$1$ optimization problems, so each coordinate $x_i$ of a feasible solution has a value at most $1$. Let $z_{S}$ be a (relaxed) indicator variable for solution $S$. If $z_{S} = a$, for some $a \in (0,1]$, then every variable $x_{i} = a$ if $i \in S$, and $x_{i} = 0$ if $i \notin S$. Otherwise, $z_S = 0$. In other words, $z_{S}$ indicates the fraction with which the solution $S$ participates in the final solution. The new formulation is visible on \cref{fig:reformulation} and its dual on \cref{fig:ref-dual}. Intuitively, $y_S^{t}$ indicates the fractional increase of the usage of solution $S$ at time $t$. The second constraint guarantees that if the linear combination of the experts' solutions sets a variable $x_i$ to a non-zero value, then every solution that includes $x_i$, must be used as well. The third constrain guarantees that we have a linear combination of the feasible solutions.

\begin{figure}[ht]
    \begin{mdframed}
        \vspace{-8pt}
        \begin{align*}
            && \min \sum_{t = 1}^{T} \sum_{S} & c_S y_S^t \\
            %
            (\alpha^{t}) \qquad && \sum_{k=1}^{K} w_{k}^{t} & \geq 1  & \forall\ t \\
            %
            (\beta_{i}^{t}) \qquad && \sum_{S: i \in S} z_{S}^{t} &\ge \sum_{k=1}^{K} s_{i,k}^{t} w_{k}^{t}   &\forall\ i,t\\
            %
            (\gamma^{t}) \qquad && \sum_{S} z_{S}^{t} &= 1   &\forall\ t\\
            %
            (\xi_{S}^{t}) \qquad && y_{S}^{t} &= z_{S}^{t} - z_{S}^{t-1}   &\forall\ S,t\\
            %
            && w_{k}^{t}, y_{S}^{t}, z_{S}^{t} & \ge 0 & \forall\ k,S,t
        \end{align*}
    \end{mdframed}
    \caption{Reformulation and relaxation of the \texttt{LIN-COMB} benchmark}
    \label{fig:reformulation}
\end{figure}

The new formulation of the \texttt{LIN-COMB} benchmark lower bounds its original formulation. According to the theorem of weak duality, the dual of the new formulation is a lower bound to the new formulation. Our proposed algorithm sets the variable $x$ of the original covering problem and the dual of the new formulation of the \texttt{LIN-COMB} benchmark. Afterwards, we show that at every time step, the increase of the objective cost in the original covering problem is bounded by the increase of the dual objective cost of the \texttt{LIN-COMB} benchmark's reformulation.

\clearpage

\begin{figure}[ht]
    \begin{mdframed}
        \vspace{-8pt}
        \begin{align*}
            && \max \sum_{t=1}^{T}  (\alpha^{t} &+ \gamma^{t}) \\
        %
            (w_{k}^{t}) \qquad && \alpha^{t} - \sum_{i=1}^{n} s_{i,k}^{t} \beta_{i}^{t} &\leq 0  &\forall\ k,t\\
        %
            (y_{S}^{t}) \qquad && \xi_{S}^{t}   &\leq c_{S}  &\forall\ S,t \\
        %
            (z_{S}^{t}) \qquad && \sum_{i \in S}\beta_{i}^{t} + \gamma^{t} - \xi_{S}^{t} + \xi_{S}^{t+1}  &\leq 0  &\forall\ S,t \\
        %
            && \alpha^{t}, \beta_{i}^{t} & \ge 0 & \forall\ S,i,t
        \end{align*}
    \end{mdframed}
    \caption{Dual formulation of the reformulation of the \texttt{LIN-COMB} benchmark}
    \label{fig:ref-dual}
\end{figure}

\subsection{Convex objective function properties} \label{sec:f-properties}

During our algorithm's description and analysis, we use the following two notions of function smoothness. We assume that the original covering problem's objective function $f$ is convex, $L$ Lipschitz-smooth, and $(\lambda, \mu)$-locally-smooth.

\begin{definition}[Lipschitz-smoothness]
    A function $f$ is Lipschitz-smooth with a constant $L$, if its derivatives are Lipschitz-continuous with the constant $L$. Formally, for all $x$ and $y$:
    \[\|\nabla f(x) - \nabla f(y)\| \le L \|x - y\|\]
\end{definition}

\begin{definition}[$(\lambda,\mu)$-local-smoothness \cite{Thang20:Online-Primal-Dual}]
    Let $\mathcal{E}$ be a set of $n$ resources.
    A differentiable function $F: [0,1]^{n} \rightarrow \mathbb{R}^{+}$ is $(\lambda,\mu)$-\emph{locally-smooth}
    if for every set $S \subseteq \mathcal{E}$, and for every set of $|S|$ arbitrary vectors $\vect{x}^{i} \in [0,1]^{n}$ where $i \in S$, the following inequality holds:
    \begin{equation*}	\label{eq:min-local-smooth}
    \sum_{i \in S} \nabla_{i} F(\vect{x}^{i}) \leq \lambda F\bigl( \vect{1}_{S} \bigr) + \mu F\bigl( \vect{x} \bigr)
    \end{equation*}
    where $\vect{x}$ is a vector whose every coordinate $x_{i'} = \max_{i}\{x^{i}_{i'}\}$ (formally, $\vect{x} := \bigvee_{i \in S} \vect{x}^{i}$);
    and $\nabla_{i} F(\vect{x})$ denotes $\partial F(\vect{x})/\partial x_{i}$.
\end{definition}

\subsection{Algorithm description}

\paragraph{Preprocessing.} We apply the preprocessing techniques of the previous algorithm to this algorithm as well, including the dummy expert to avoid division by zero. For details see \cref{sec:algo}.

\clearpage

\paragraph{Algorithm 2.} \label{algo-convex}

At the arrival of the $t^{\text{th}}$ constraint,
\begin{compactenum}
\item solve the following non-linear program and set $w^t$ to be the obtained locally optimal solution

\begin{align*}
\min_{w} \biggl\{\sum_{i=1}^{n}
\nabla_{i} & f\bigl(x^{t-1} \bigr) \biggl[  \biggl(\sum_{k=1}^{K} s_{i,k}^{t} w_{i,k}  + \delta_{i}^{t} \biggr)
\ln \left(\frac{ \sum_{k=1}^{K} s_{i,k}^{t} w_{i,k}  + \delta_{i}^{t} }{\sum_{k=1}^{K} s_{i,k}^{t-1} w_{i,k}^{t-1} + \delta_{i}^{t-1}}\right)
- \sum_{k=1}^{K}  s_{i,k}^{t} w_{i,k} \biggr] \\
%
&+ \frac{L}{2}\biggl( \sum_{k=1}^{K} s_{i,k}^{t} w_{i,k}  - 2x_{i}^{t-1} - \delta_{i}^{t} \biggr) \biggl(\sum_{k=1}^{K} s_{i,k}^{t} w_{i,k} + \delta_{i}^{t} \biggr)
\ln \left(\frac{ \sum_{k=1}^{K} s_{i,k}^{t} w_{i,k}  + \delta_{i}^{t} }{\sum_{k=1}^{K} s_{i,k}^{t-1} w_{i,k}^{t-1}  + \delta_{i}^{t-1}}\right) \\
%
&- \frac{L}{4} \biggl( \sum_{k=1}^{K} s_{i,k}^{t} w_{i,k}  - 2x_{i}^{t-1} - \delta_{i}^{t} \biggr)^{2}
\biggr\}
\end{align*}
%
\noindent subject to:
%
\begin{align*}
    (\theta^{t})  && \sum_{i=1}^{n} a_{i}^{t} \biggl( \sum_{k=1}^{K}  \hat{s}_{i,k}^{t} w_{i,k} \biggr) &\geq 1 && \forall\ t\\
    %
    (\chi_{i}) && \sum_{k=1}^{K}  w_{i,k} &\geq 1 && \forall\ i\\
    %
    (\nu_{i}^{t}) && \sum_{k=1}^{K} s_{i,k}^{t} w_{i,k} &\geq 0 && \forall\ i,t
\end{align*}
%
where $\delta_{i}^{t} = \frac{1}{K} \sum_{k} s_{i,k}^{t}$ and $L$ is the Lipschitz-smoothness constant of $f$.
Note that in this program, we use the auxiliary solution $\hat{s}_{i,k}^{t}$ in the first constraint. For every $i$ where $s_{i,k}^{t} = 0$ for all $k$, the term related to $i$ is not included in the objective function of the convex program.
(We can set $w_{i,k} = 0$ for all $k$ beforehand.)
%
\item For all $i$, if $\sum_{k} w_{i,k}^{t} s_{i,k}^{t} > x_{i}^{t-1}$ then set $x_{i}^{t} \gets \sum_{k} w_{i,k}^{t} s_{i,k}^{t}$;
otherwise set $x_{i}^{t} \gets x_{i}^{t-1}$.
\end{compactenum}

\subsection{Performance analysis}
As $w^{t}$ is the locally optimal solution of the non-linear program and ($\theta^t,\ \chi_{i},\ \nu_{i}^{t}$) is the locally optimal solution of its dual, the following Karush-Kuhn-Tucker (KKT) and complementary slackness conditions hold.

\begin{align*}
   \biggl[ \sum_{i=1}^{n} a_{i}^{t} \biggl( \sum_{k=1}^{K}  \hat{s}_{i,k}^{t} w_{i,k}^{t} \biggr) - 1 \biggr] \theta^{t} &= 0 \qquad \forall \ t \\
   \biggl[ \sum_{k=1}^{K}  w_{i,k}^{t}  - 1 \biggr] \chi_{i} &= 0 \qquad \forall\ i, t \\
   \biggl[ \sum_{k=1}^{K}  s_{i,k}^{t} w_{i,k}^{t} \biggr] \nu_{i}^{t} &= 0 \qquad \forall\ i, t \\
%
 s_{i,k}^{t} \biggl[  \nabla_{i} f(x^{t-1}) + L\biggl( \sum_{k=1}^{K} s_{i,k}^{t} w_{i,k}^{t} - x_{i}^{t-1} \biggr) \biggr] \ln \left( \frac{\sum_{k=1}^{K} s_{i,k}^{t} w_{i,k}^{t} + \delta_{i}^{t}}{\sum_{k=1}^{K}  s_{i,k}^{t-1}w_{i,k}^{t-1}  + \delta_{i}^{t-1}} \right) \\
        - a_{i}^{t} \hat{s}_{i,k}^{t} \theta^{t} - \chi_{i} - s_{i,k}^{t} \nu_{i}^{t} &= 0	\qquad \forall\ i,k,t \\
    %
    \theta^{t}, \chi_{i}, \nu_{i}^{t} &\geq 0 \qquad \forall\ i, t
\end{align*}

Moreover, if $\sum_{k} w_{i,k}^{t} s_{i,k}^{t} > 0$, meaning that $\nu_{i}^{t} = 0$, then
\begin{align}	\label{eq:KKT2}
 s_{i,k}^{t} \biggl[  \nabla_{i} f(x^{t-1}) + L\biggl( \sum_{k=1}^{K} s_{i,k}^{t} w_{i,k}^{t}  - x_{i}^{t-1} \biggr) \biggr] \ln \left( \frac{\sum_{k=1}^{K} s_{i,k}^{t} w_{i,k}^{t} + \delta_{i}^{t}}{\sum_{k=1}^{K}  s_{i,k}^{t-1}w_{i,k}^{t-1}  + \delta_{i}^{t-1}} \right)
        - a_{i}^{t} \hat{s}_{i,k}^{t} \theta^{t} - \chi_{i}^{t} = 0
\end{align}

\clearpage

\paragraph{Dual variables and feasibility.} We set the dual variables of the reformulation of our \texttt{LIN-COMB} benchmark based on the dual variables of the non-linear program used inside the algorithm. The dual formulation is visible on \cref{fig:ref-dual}. The constants $\lambda$ and $\mu$ correspond to the $(\lambda,\mu)$-local-smoothness, and $L$ to the Lipschitz-smoothness of $f$ (see the definitions at \cref{sec:f-properties}).
%
\begin{align*}
    \alpha^{t} &= \frac{1}{\lambda}  \biggl( \theta^{t} + \sum_{i=1}^{n} \chi_{i} \biggr), \\
    %
    \beta_{i}^{t} &= \frac{1}{\lambda} \biggl[  \nabla_{i} f(x^{t-1}) + L\biggl( \sum_{k=1}^{K} s_{i,k}^{t} w_{i,k}^{t}  - x_{i}^{t-1} \biggr) \biggr] \ln \left( \frac{\sum_{k=1}^{K} s_{i,k}^{t} w_{i,k}^{t} + \delta_{i}^{t}}{\sum_{k=1}^{K}  s_{i,k}^{t-1}w_{i,k}^{t-1}  + \delta_{i}^{t-1}} \right) \\
    %
    \xi_{S}^{t} &= \frac{1}{\lambda} \sum_{i \in S} \nabla_{i} f(x^{t-1}) \ln \left( \frac{(1 + 1/K) \cdot \max_{t'} \sum_{k=1}^{K} s_{i,k}^{t'}}{\sum_{k=1}^{K}  s_{i,k}^{t-1}w_{i,k}^{t-1}  + \delta_{i}^{t-1}} \right) - \frac{\mu}{\lambda} \ln(K\rho)  f(x^{t-1})\\
    %
    \gamma^{t} &= -  \frac{1}{\lambda} \sum_{i=1}^{n} \biggl[  \nabla_{i} f(x^{t-1}) + L\biggl( \sum_{k=1}^{K} s_{i,k}^{t} w_{i,k}^{t}  - x_{i}^{t-1} \biggr) \biggr]  \ln \left( \frac{\sum_{k=1}^{K} s_{i,k}^{t} w_{i,k}^{t} + \delta_{i}^{t}}{\sum_{k=1}^{K}  s_{i,k}^{t-1}w_{i,k}^{t-1}  + \delta_{i}^{t-1}} \right) \\
        & \quad - \frac{1}{\lambda} \sum_{i=1}^{n} \biggl[ \nabla_{i} f(x^{t}) \ln \left( \frac{(1 + 1/K) \cdot \max_{t'} \sum_{k=1}^{K} s_{i,k}^{t'}}{\sum_{k=1}^{K}  s_{i,k}^{t}w_{i,k}^{t}  + \delta_{i}^{t}} \right) -  \nabla_{i} f(x^{t-1}) \ln \left( \frac{(1 + 1/K) \cdot \max_{t'} \sum_{k=1}^{K} s_{i,k}^{t'}}{\sum_{k=1}^{K}  s_{i,k}^{t-1}w_{i,k}^{t-1}  + \delta_{i}^{t-1}} \right) \biggr] \\
    & \quad + \frac{\mu}{\lambda} \ln(K\rho) f(x^{t}) - \frac{\mu}{\lambda} \ln(K\rho) f(x^{t-1})
\end{align*}
%
Recall that $\rho = \max_{i, t',t''} \left\{\frac{\sum_{k=1}^{K} s_{i,k}^{t'}}{\sum_{k=1}^{K} s_{i,k}^{t''}} : \sum_{k=1}^{K} s_{i,k}^{t''} > 0 \right\}$.

\begin{lemma} \label{lem:convex-covering-feasibility}
The original covering problem's $x_{i}^{t}$ solution set by the algorithm and the dual variables $(\alpha^{t}, \beta_{i}^{t}, \xi_S^t, \gamma^t)$ of the reformulated \texttt{LIN-COMB} benchmark are feasible up to a factor of $\ln (K\rho)$.
\end{lemma}
%
\begin{proof}
We first prove that the $x_{i}^{t}$ variables satisfy the covering constraints by induction. At time~0, no constraint has been released yet, and every variable is set to 0. This all-zero solution is feasible. Let us assume that the algorithm provides feasible solutions up to time $t-1$. At time~$t$, the algorithm maintains the inequality $x_{i}^{t} \geq x_{i}^{t-1}$, so all constraints $t'$ where $t' < t$ are satisfied. Besides, $x_{i}^{t}$ is always at least
$\sum_{k} w_{i,k}^{t} s_{i,k}^{t}$, which is larger than $\sum_{k} w_{i,k}^{t} \hat{s}_{i,k}^{t}$ since $s_{i,k}^{t} \geq \hat{s}_{i,k}^{t}$
for all $i,k$ due to the preprocessing step. Hence, constraint $t$ is also satisfied, formally,
$$
\sum_{i=1}^{n} a_{i}^{t} x_{i}^{t}  \geq \sum_{i=1}^{n} a_{i}^{t} \biggl( \sum_{k=1}^{K} \hat{s}_{i,k}^{t} w_{i,k}^{t} \biggr) \geq 1.
$$

In the remaining part of the proof, we show the feasibility of the dual solution. The dual constraints are visible on \cref{fig:ref-dual}.
The first constraint follows from the KKT conditions and from the fact that $\sum_{i} a_i^{t} \hat{s}_{i,k}^{t} = 1$.
The second constraint is satisfied up to a factor of $\ln (K\rho)$ because
%
\begin{align*}
\xi_{S}^{t} &= \frac{1}{\lambda} \sum_{i \in S} \nabla_{i} f(x^{t-1}) \ln \left( \frac{(1 + 1/K) \cdot \max_{t'} \sum_{k=1}^{K} s_{i,k}^{t'}}{\sum_{k=1}^{K}  s_{i,k}^{t-1}w_{i,k}^{t-1}  + \delta_{i}^{t-1}} \right) - \frac{\mu}{\lambda} \ln(K\rho) f(x^{t-1})  \\
&\leq \frac{1}{\lambda} \sum_{i \in S} \nabla_{i} f(x^{t-1}) \ln(K\rho)  - \frac{\mu}{\lambda} \ln(K\rho) f(x^{t-1})
\leq  \ln(K\rho) c_S
\end{align*}
%
where the last inequality holds due to the $(\lambda,\mu)$-local-smoothness. Important: after down-scaling the variables by a factor of $\ln (K\rho)$, the second dual constraint holds.

The last constraint is satisfied due to the definition of the dual variables. We need
\begin{align*}
  \sum_{i \in S}\beta_{i}^{t} + \gamma^{t} - \xi_{S}^{t} + \xi_{S}^{t+1}  &\leq 0\\
  \sum_{i \in S}\beta_{i}^{t} - \xi_{S}^{t} + \xi_{S}^{t+1}  &\leq -\gamma^{t}
\end{align*}
%
to hold. We can simplify $\gamma^{t}$ as follows:
\begin{align*}
    \gamma^{t} &= -  \frac{1}{\lambda} \sum_{i=1}^{n} \biggl[ L\biggl( \sum_{k=1}^{K} s_{i,k}^{t} w_{i,k}^{t}  - x_{i}^{t-1} \biggr) \biggr]  \ln \left( \frac{\sum_{k=1}^{K} s_{i,k}^{t} w_{i,k}^{t} + \delta_{i}^{t}}{\sum_{k=1}^{K}  s_{ik}^{t-1}w_{i,k}^{t-1}  + \delta_{i}^{t-1}} \right) \\
    	& \quad - \frac{1}{\lambda} \sum_{i=1}^{n} \biggl[ \nabla_{i} f(x^{t}) - \nabla_{i} f(x^{t-1}) \biggr] \ln \left( \frac{(1 + 1/K) \cdot \max_{t'} \sum_{k=1}^{K} s_{i,k}^{t'}}{\sum_{k=1}^{K}  s_{ik}^{t}w_{i,k}^{t}  + \delta_{i}^{t}} \right)  \\
	& \quad + \frac{\mu}{\lambda} \ln(K\rho) f(x^{t}) - \frac{\mu}{\lambda} \ln(K\rho) f(x^{t-1})
\end{align*}
%
Now let us fix an arbitrary set $S$. Then, following the simplified form of $\gamma^t$, we have
\begin{align*}
    -\gamma^{t} &\geq     \frac{1}{\lambda} \sum_{i \in S} \biggl[ L\biggl( \sum_{k=1}^{K} s_{i,k}^{t} w_{i,k}^{t}  - x_{i}^{t-1} \biggr) \biggr]  \ln \left( \frac{\sum_{k=1}^{K} s_{i,k}^{t} w_{i,k}^{t} + \delta_{i}^{t}}{\sum_{k=1}^{K}  s_{ik}^{t-1}w_{i,k}^{t-1}  + \delta_{i}^{t-1}} \right) \\
            & \quad + \frac{1}{\lambda} \sum_{i \in S} \biggl[ \nabla_{i} f(x^{t}) - \nabla_{i} f(x^{t-1}) \biggr] \ln \left( \frac{(1 + 1/K) \cdot \max_{t'} \sum_{k=1}^{K} s_{i,k}^{t'}}{\sum_{k=1}^{K}  s_{ik}^{t}w_{i,k}^{t}  + \delta_{i}^{t}} \right)  \\
        & \quad - \frac{\mu}{\lambda} \ln(K\rho) f(x^{t}) + \frac{\mu}{\lambda} \ln(K\rho) f(x^{t-1})
\end{align*}
because $\nabla_{i} f(x^{t}) \geq \nabla_{i} f(x^{t-1})$ due to convexity of $f$, the logarithm terms are always positive, and we restrict the sum to a subset.
%
We have to show that the following is true to satisfy the third constraint:
\begin{align*}
    \sum_{i \in S}\beta_{i}^{t} - \xi_{S}^{t} + \xi_{S}^{t+1}  & = \sum_{i \in S} \frac{1}{\lambda} \biggl[  \nabla_{i} f(x^{t-1}) + L\biggl( \sum_{k=1}^{K} s_{i,k}^{t} w_{i,k}^{t}  - x_{i}^{t-1} \biggr) \biggr] \ln \left( \frac{\sum_{k=1}^{K} s_{i,k}^{t} w_{i,k}^{t} + \delta_{i}^{t}}{\sum_{k=1}^{K}  s_{i,k}^{t-1}w_{i,k}^{t-1}  + \delta_{i}^{t-1}} \right) \\
        & \quad - \frac{1}{\lambda} \sum_{i \in S} \nabla_{i} f(x^{t-1}) \ln \left( \frac{(1 + 1/K) \cdot \max_{t'} \sum_{k=1}^{K} s_{i,k}^{t'}}{\sum_{k=1}^{K}  s_{i,k}^{t-1}w_{i,k}^{t-1}  + \delta_{i}^{t-1}} \right) + \frac{\mu}{\lambda} \ln(K\rho)  f(x^{t-1}) \\
        & \quad + \frac{1}{\lambda} \sum_{i \in S} \nabla_{i} f(x^{t}) \ln \left( \frac{(1 + 1/K) \cdot \max_{t'} \sum_{k=1}^{K} s_{i,k}^{t'}}{\sum_{k=1}^{K}  s_{i,k}^{t}w_{i,k}^{t}  + \delta_{i}^{t}} \right) - \frac{\mu}{\lambda} \ln(K\rho)  f(x^{t}) \\
    & =     \frac{1}{\lambda} \sum_{i \in S} \biggl[ L\biggl( \sum_{k=1}^{K} s_{i,k}^{t} w_{i,k}^{t}  - x_{i}^{t-1} \biggr) \biggr]  \ln \left( \frac{\sum_{k=1}^{K} s_{i,k}^{t} w_{i,k}^{t} + \delta_{i}^{t}}{\sum_{k=1}^{K}  s_{ik}^{t-1}w_{i,k}^{t-1}  + \delta_{i}^{t-1}} \right) \\
            & \quad + \frac{1}{\lambda} \sum_{i \in S} \biggl[ \nabla_{i} f(x^{t}) - \nabla_{i} f(x^{t-1}) \biggr] \ln \left( \frac{(1 + 1/K) \cdot \max_{t'} \sum_{k=1}^{K} s_{i,k}^{t'}}{\sum_{k=1}^{K}  s_{ik}^{t}w_{i,k}^{t}  + \delta_{i}^{t}} \right)  \\
            & \quad - \frac{\mu}{\lambda} \ln(K\rho) f(x^{t}) + \frac{\mu}{\lambda} \ln(K\rho) f(x^{t-1}) \\
    & \leq - \gamma^t
\end{align*}
where the second equality holds due to $\log(a/b) - \log(a/c) = \log(c/b)$.

It remains to show that $\alpha^t$ and $\beta_i^t$ are positive. Due to the KKT conditions both variables are always positive, so the lemma holds.
\end{proof}

\clearpage

\begin{theorem} \label{convex-covering-theorem}
Algorithm 2 is $O(\ln(K \rho)) \frac{\lambda}{1 - \mu \ln (K\rho)}$-competitive with the \texttt{LIN-COMB} benchmark.
\end{theorem}
%
\begin{proof} \cref{lem:convex-covering-feasibility} proved that our algorithm creates feasible solutions for the original covering problem and for the dual problem of the reformulated \texttt{LIN-COMB} benchmark. We show that the algorithm's solution increases the primal objective value of the original covering problem by at most $O(\ln(K \rho))$ times the value of the dual solution, which serves as the lower bound on the \texttt{LIN-COMB} benchmark - the best linear combination of the experts' solutions.

\begin{align}
&f(x^{t}) - f(x^{t-1}) \leq \nabla f(x^{t-1}) (x^{t} - x^{t-1}) + \frac{2L}{2} \|x^{t} - x^{t-1} \|^{2} \\
%
&\leq \sum_{i: x_{i}^{t} > x_{i}^{t-1}} \bigl[ \nabla_{i} f(x^{t-1}) + L(x_{i}^{t} - x_{i}^{t-1}) \bigr] (x_{i}^{t} - x_{i}^{t-1}) &&  \notag \\
%
&\leq \sum_{i: x_{i}^{t} > x_{i}^{t-1}} \bigl[ \nabla_{i} f(x^{t-1}) + L(x_{i}^{t} - x_{i}^{t-1}) \bigr](x_{i}^{t} + \delta_{i}^{t}) \ln \left(\frac{x_{i}^{t} + \delta_{i}^{t}}{x_{i}^{t-1} + \delta_{i}^{t}} \right) \\
%
&\leq \sum_{i: x_{i}^{t} > x_{i}^{t-1}} \bigl[ \nabla_{i} f(x^{t-1}) + L(x_{i}^{t} - x_{i}^{t-1}) \bigr] (x_{i}^{t} + \delta_{i}^{t}) \ln \left(\frac{x_{i}^{t} + \delta_{i}^{t}}{x_{i}^{t-1} + \delta_{i}^{t-1}} \right) \\
%
&= \sum_{i: x_{i}^{t} > x_{i}^{t-1}} \bigl[ \nabla_{i} f(x^{t-1}) + L(x_{i}^{t} - x_{i}^{t-1}) \bigr] \left[ \left(\sum_{k=1}^{K}  s_{i,k}^{t} w_{i,k}^{t} + \frac{1}{K} \sum_{k=1}^{K} s_{i,k}^{t} \right)
            \ln \left(\frac{ \sum_{k=1}^{K}  s_{i,k}^{t} w_{i,k}^{t} + \delta_{i}^{t}}{x_{i}^{t-1} + \delta_{i}^{t-1}}  \right) \right]\\
%
&\leq \sum_{i: x_{i}^{t} > x_{i}^{t-1}} \bigl[ \nabla_{i} f(x^{t-1}) + L(x_{i}^{t} - x_{i}^{t-1}) \bigr] \left[ \left(\sum_{k=1}^{K}  s_{i,k}^{t} w_{i,k}^{t} + \frac{1}{K} \sum_{k=1}^{K} s_{i,k}^{t} \right)
            \ln \left(\frac{ \sum_{k=1}^{K}  s_{i,k}^{t} w_{i,k}^{t} + \delta_{i}^{t}}{\sum_{k=1}^{K}  s_{i,k}^{t-1} w_{i,k}^{t-1} + \delta_{i}^{t-1}}  \right) \right]\\
%
&= \sum_{i: x_{i}^{t} > x_{i}^{t-1}} \sum_{k=1}^{K} (w_{i,k}^{t} + 1/K) s_{i,k}^{t} \bigl[ \nabla_{i} f(x^{t-1}) + L(x_{i}^{t} - x_{i}^{t-1}) \bigr]
            \ln \left(\frac{ \sum_{k=1}^{K} s_{i,k}^{t} w_{i,k}^{t}  + \delta_{i}^{t}}{\sum_{k=1}^{K}  s_{i,k}^{t-1} w_{i,k}^{t-1}  + \delta_{i}^{t-1}}  \right) \notag \\
%
&=  \sum_{i: x_{i}^{t} > x_{i}^{t-1}} \sum_{k=1}^{K} (w_{i,k}^{t} + 1/K) \biggl( a_{i}^{t} \hat{s}_{i,k}^{t} \theta^{t} + \chi_{i}^{t} \biggr)  \\
%
&\leq \sum_{i=1}^{n} \sum_{k=1}^{K} (w_{i,k}^{t} + 1/K) \biggl( a_{i}^{t} \hat{s}_{i,k}^{t} \theta^{t} + \chi_{i}^{t} \biggr) \notag \\
%
&= \sum_{i=1}^{n} a_{i}^{t} \biggl(\sum_{k=1}^{K} w_{i,k}^{t} \hat{s}_{i,k}^{t} \biggr) \theta^t + \sum_{i=1}^{n} \biggl( \sum_{k=1}^{K} w_{i,k}^{t} \biggr) \chi_{i}^{t}
+ \frac{1}{K}  \sum_{k=1}^{K} \biggl( \sum_{i=1}^{n} a_{i}^{t}  \hat{s}_{i,k}^{t}  \biggr) \theta^t + \frac{1}{K} \sum_{k=1}^{K} \sum_{i=1}^{n} \chi_{i}^{t} 		\notag \\
%
&= 2 \theta^{t} + 2\sum_{i=1}^{n} \chi_{i}^{t} \\
%
&= 2\lambda \cdot \alpha^{t}	\notag
\end{align}
%
The above corresponding transformations hold since:
\begin{compactenum}[(1)]
    \setcounter{enumi}{8}
    \item holds as $f$ is $2L$-smooth;
    \item follows from the inequality $a - b \leq a \ln(a/b)$ for all $0 < b \leq a$;
    \item holds since $\delta_{i}^{t} \geq \delta_{i}^{t-1}$ (because $s_{i,k}^{t} \geq s_{i,k}^{t-1}$ for all $i,k,t$);
    \item is valid because $x_{i}^{t} > x_{i}^{t-1}$, so $x_{i}^{t} = \sum_{k=1}^{K}  s_{i,k}^{t} w_{i,k}^{t}$;
    \item is by the design of the algorithm: $x_{i}^{t-1} \geq \sum_{k=1}^{K}  s_{i,k}^{t-1} w_{i,k}^{t-1}$;
    %\setcounter{enumi}{5}
    \item since given that $x_{i}^{t} > x_{i}^{t-1} \geq 0$
    (so $\sum_{k=1}^{K}  s_{i,k}^{t} w_{i,k}^{t} = x_{i}^{t} > 0$), the KKT condition (\ref{eq:KKT2}) applies;
    \item is true due to the complementary slackness conditions
        and that $\sum_{i=1}^{n} a_{i}^{t}  \hat{s}_{i,k}^{t} = 1$.
\end{compactenum}

\clearpage

\noindent The objective of the dual includes $\sum_{t = 1}^{T} \gamma^{t}$ as well, so it remains to bound this sum. Recall the simplified version of $\gamma^t$:
\begin{align*}
    \gamma^{t} &= -  \frac{1}{\lambda} \sum_{i=1}^{n} \biggl[ L\biggl( \sum_{k=1}^{K} s_{i,k}^{t} w_{i,k}^{t}  - x_{i}^{t-1} \biggr) \biggr]  \ln \left( \frac{\sum_{k=1}^{K} s_{i,k}^{t} w_{i,k}^{t} + \delta_{i}^{t}}{\sum_{k=1}^{K}  s_{ik}^{t-1}w_{i,k}^{t-1}  + \delta_{i}^{t-1}} \right) \\
    	& \quad - \frac{1}{\lambda} \sum_{i=1}^{n} \biggl[ \nabla_{i} f(x^{t}) - \nabla_{i} f(x^{t-1}) \biggr] \ln \left( \frac{(1 + 1/K) \cdot \max_{t'} \sum_{k=1}^{K} s_{i,k}^{t'}}{\sum_{k=1}^{K}  s_{ik}^{t}w_{i,k}^{t}  + \delta_{i}^{t}} \right)  \\
	& \quad + \frac{\mu}{\lambda} \ln(K\rho) f(x^{t}) - \frac{\mu}{\lambda} \ln(K\rho) f(x^{t-1})
\end{align*}
%
Since $f$ is $2L$-smooth with respect to the $\ell_{2}$-norm, we can observe that
\begin{align*}
\sum_{i=1}^{n} (\nabla_{i} f(x^{t}) -  \nabla_{i} f(x^{t-1}) \ln \left( \frac{(1 + 1/K) \cdot \max_{t'} \sum_{k=1}^{K} s_{i,k}^{t'}}{\sum_{k=1}^{K}  s_{i,k}^{t}w_{i,k}^{t}  + \delta_{i}^{t}} \right)
\leq \ln(K\rho) 2L\sqrt{n} \sum_{i=1}^{n} (x_{i}^{t} - x_{i}^{t-1})
\end{align*}
%
and after bounding the following term
%
\begin{align*}
2\biggl( \sum_{k=1}^{K} s_{i,k}^{t} w_{i,k}^{t}  - x_{i}^{t-1} \biggr) \ln \left( \frac{\sum_{k=1}^{K} s_{i,k}^{t} w_{i,k}^{t} + \delta_{i}^{t}}{\sum_{k=1}^{K}  s_{i,k}^{t-1}w_{i,k}^{t-1}  + \delta_{i}^{t-1}} \right)
\leq \ln(K\rho) (x_{i}^{t} - x_{i}^{t-1})
\end{align*}
%
we get that
\begin{align*}
\sum_{t=1}^{T} \gamma^{t} &\geq - \frac{\mu \ln (K\rho)}{\lambda} f(x^{T}) - \ln(K\rho) \frac{4L}{\lambda} \sqrt{n} \sum_{t=1}^{T} \sum_{i=1}^{n} (x_{i}^{t} - x_{i}^{t-1}) \\
&\geq - \frac{\mu \ln (K\rho)}{\lambda} f(x^{T}) - \ln(K\rho) \frac{4L}{\lambda} \sqrt{n} \sum_{i=1}^{n} x_{i}^{T}.
\end{align*}
Considering both parts of the proof so far, we bound the dual objective as follows:
\begin{align*}
\sum_{t=1}^{T} (\alpha^{t} + \gamma^{t}) \geq \frac{1/2-\mu\ln (K\rho)}{\lambda} f(x^{T}) - \ln(K\rho) \frac{4L}{\lambda} \sqrt{n} \sum_{i=1}^{n} x_{i}^{T}
\end{align*}
In $0-1$ optimization, $x_{i}^{T} \leq 1$.
Hence, the theorem follows.
\end{proof}
