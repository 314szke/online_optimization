%!TEX root = ./main.tex

\section{Conclusion} \label{sec:conclusion}

We introduce a dynamic \texttt{LIN-COMB} benchmark in the setting of multiple expert predictions, which goes beyond the traditional static best expert in hindsight benchmark. We give two competitive algorithms for the online linear and convex covering problems in this benchmark.

By \cref{corollary}, given a $0$-$1$ optimization problem, if there are $K$ deterministic online algorithms, then
we can design an algorithm that has a cost at most $O(\log (K))$ times that of the best linear combination of those algorithms at any time.
Similarly, if $K$ given online algorithms are randomized (they output $0$-$1$ solutions with probabilities), then our algorithm
has an expected cost (randomization over the product of the distributions of those solutions) at most $O(\log(K))$ times that of
the best linear combination of those algorithms at any time. Many practical problems admit $0$-$1$ solutions, for which our algorithm is of interest.
Consider problems like network design, ski rental, TCP acknowledgement, facility location, and so on. Given the fractional solution of our algorithm,
we can apply existing online rounding schemes to obtain integral solutions for such problems.


\section{Ethical perspective}

This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.
