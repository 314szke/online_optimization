%!TEX root = ./main.tex

\begin{abstract}
    Designing online algorithms with machine learning predictions is a recent approach beyond the worst-case paradigm for various practically relevant online problems. (For example: scheduling, caching, clustering, and ski rental.) While most previous learning-augmented algorithms focus on integrating the predictions of a single oracle,
    we study the design of online algorithms with \emph{multiple} prediction sources (experts). To go beyond the performance guarantee of the popular static best expert in hindsight benchmark, we compare are algorithm to a new \emph{dynamic} benchmark (the linear combination of predictions that change over time).
    We present a competitive algorithm in the new dynamic benchmark for $0$-$1$ online linear covering problems with a performance guarantee of $O(\ln(K))$, where $K$ is the number of experts. To take a step further towards a general framework, we also show an algorithm for $0$-$1$ online \emph{convex} covering with a similar performance guarantee of $O(\ln(K)) \cdot \frac{\lambda}{(1-\mu\ln(K))}$, where $\lambda$ and $\mu$ are the ($\lambda$-$\mu$)-smoothness parameters of the objective function. Furthermore, our multiple-expert approach provides a new perspective on how to combine several online algorithms in an online manner - a central subject in the online algorithm research community.
\end{abstract}
